#!/usr/bin/env python3
"""
é¸¢å°¾èŠ±æ•°æ®é›†æ•°æ®å¯¼å‡ºè„šæœ¬
å°†æ•°æ®é›†ä¿¡æ¯ä¿å­˜åˆ°æ–‡ä»¶ä¸­
"""

import os
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris

def create_data_folder():
    """åˆ›å»ºæ•°æ®æ–‡ä»¶å¤¹"""
    data_dir = "ML/data"
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        print(f"âœ… åˆ›å»ºæ–‡ä»¶å¤¹: {data_dir}")
    return data_dir

def save_iris_data():
    """ä¿å­˜é¸¢å°¾èŠ±æ•°æ®é›†åˆ°æ–‡ä»¶"""
    # åŠ è½½æ•°æ®
    iris = load_iris()
    
    # åˆ›å»ºæ•°æ®æ–‡ä»¶å¤¹
    data_dir = create_data_folder()
    
    # 1. ä¿å­˜åŸå§‹æ•°æ®ä¸ºCSV
    df = pd.DataFrame(iris.data, columns=iris.feature_names)
    df['target'] = iris.target
    df['target_name'] = [iris.target_names[i] for i in iris.target]
    
    csv_file = os.path.join(data_dir, "iris_data.csv")
    df.to_csv(csv_file, index=False)
    print(f"âœ… ä¿å­˜CSVæ–‡ä»¶: {csv_file}")
    
    # 2. ä¿å­˜æ•°æ®ä¿¡æ¯ä¸ºæ–‡æœ¬æ–‡ä»¶
    info_file = os.path.join(data_dir, "iris_info.txt")
    with open(info_file, 'w', encoding='utf-8') as f:
        f.write("é¸¢å°¾èŠ±æ•°æ®é›†ä¿¡æ¯\n")
        f.write("=" * 50 + "\n\n")
        
        f.write("æ•°æ®é›†æ¦‚è¿°:\n")
        f.write(f"- æ ·æœ¬æ•°é‡: {iris.data.shape[0]}\n")
        f.write(f"- ç‰¹å¾æ•°é‡: {iris.data.shape[1]}\n")
        f.write(f"- ç±»åˆ«æ•°é‡: {len(iris.target_names)}\n\n")
        
        f.write("ç‰¹å¾åç§°:\n")
        for i, name in enumerate(iris.feature_names):
            f.write(f"{i+1}. {name}\n")
        f.write("\n")
        
        f.write("ç±»åˆ«åç§°:\n")
        for i, name in enumerate(iris.target_names):
            f.write(f"{i}. {name}\n")
        f.write("\n")
        
        f.write("æ•°æ®ç»Ÿè®¡ä¿¡æ¯:\n")
        f.write("-" * 30 + "\n")
        for i, feature in enumerate(iris.feature_names):
            f.write(f"{feature}:\n")
            f.write(f"  æœ€å°å€¼: {iris.data[:, i].min():.2f}\n")
            f.write(f"  æœ€å¤§å€¼: {iris.data[:, i].max():.2f}\n")
            f.write(f"  å¹³å‡å€¼: {iris.data[:, i].mean():.2f}\n")
            f.write(f"  æ ‡å‡†å·®: {iris.data[:, i].std():.2f}\n\n")
        
        f.write("ç±»åˆ«åˆ†å¸ƒ:\n")
        f.write("-" * 30 + "\n")
        unique, counts = np.unique(iris.target, return_counts=True)
        for i, (target, count) in enumerate(zip(unique, counts)):
            f.write(f"{iris.target_names[target]}: {count} ä¸ªæ ·æœ¬\n")
    
    print(f"âœ… ä¿å­˜ä¿¡æ¯æ–‡ä»¶: {info_file}")
    
    # 3. ä¿å­˜åŸå§‹æ•°æ®ä¸ºnumpyæ ¼å¼
    np_file = os.path.join(data_dir, "iris_data.npy")
    np.save(np_file, iris.data)
    print(f"âœ… ä¿å­˜numpyæ–‡ä»¶: {np_file}")
    
    # 4. ä¿å­˜ç›®æ ‡æ ‡ç­¾
    target_file = os.path.join(data_dir, "iris_target.npy")
    np.save(target_file, iris.target)
    print(f"âœ… ä¿å­˜ç›®æ ‡æ–‡ä»¶: {target_file}")
    
    # 5. ä¿å­˜å…ƒæ•°æ®
    metadata_file = os.path.join(data_dir, "iris_metadata.json")
    import json
    metadata = {
        "feature_names": iris.feature_names.tolist(),
        "target_names": iris.target_names.tolist(),
        "data_shape": iris.data.shape,
        "description": "é¸¢å°¾èŠ±æ•°æ®é›† - ç»å…¸çš„æœºå™¨å­¦ä¹ æ•°æ®é›†"
    }
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"âœ… ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶: {metadata_file}")
    
    return data_dir

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ¸ é¸¢å°¾èŠ±æ•°æ®é›†å¯¼å‡ºå·¥å…·")
    print("=" * 50)
    
    try:
        data_dir = save_iris_data()
        
        print("\nğŸ‰ æ•°æ®å¯¼å‡ºå®Œæˆ!")
        print(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {data_dir}")
        print("\nğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("  - iris_data.csv (CSVæ ¼å¼æ•°æ®)")
        print("  - iris_info.txt (æ•°æ®ä¿¡æ¯)")
        print("  - iris_data.npy (numpyæ•°æ®)")
        print("  - iris_target.npy (ç›®æ ‡æ ‡ç­¾)")
        print("  - iris_metadata.json (å…ƒæ•°æ®)")
        
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")

if __name__ == "__main__":
    main()
